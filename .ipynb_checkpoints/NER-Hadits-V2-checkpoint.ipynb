{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from seqeval.metrics import classification_report as seq_met\n",
    "from sklearn.metrics import classification_report as skl_met\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hadits Muslim Nomor 1</td>\n",
       "      <td>Dan</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hadits Muslim Nomor 1</td>\n",
       "      <td>ia</td>\n",
       "      <td>PRON</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hadits Muslim Nomor 1</td>\n",
       "      <td>merupakan</td>\n",
       "      <td>VERB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hadits Muslim Nomor 1</td>\n",
       "      <td>atsar</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hadits Muslim Nomor 1</td>\n",
       "      <td>yang</td>\n",
       "      <td>PRON</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title      token   pos entity\n",
       "0  Hadits Muslim Nomor 1        Dan  CONJ      O\n",
       "1  Hadits Muslim Nomor 1         ia  PRON      O\n",
       "2  Hadits Muslim Nomor 1  merupakan  VERB      O\n",
       "3  Hadits Muslim Nomor 1      atsar  NOUN      O\n",
       "4  Hadits Muslim Nomor 1       yang  PRON      O"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Datasets/100_dataset - 100.csv\", encoding = \"ISO-8859-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-LOC</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-PER</td>\n",
       "      <td>1125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I-LOC</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I-PER</td>\n",
       "      <td>2330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O</td>\n",
       "      <td>10367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity  counts\n",
       "0  B-LOC      38\n",
       "1  B-PER    1125\n",
       "2  I-LOC       9\n",
       "3  I-PER    2330\n",
       "4      O   10367"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('entity').size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    def __init__(self, data):\n",
    "        self.n_sent= 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "#         agg_func_x = lambda s: [(s.name, t, p, e, pr) for t, p, e, pr in zip(s['token'].values.tolist(), \n",
    "#                                                            s['pos'].values.tolist(), \n",
    "#                                                            s['entity'].values.tolist(), \n",
    "#                                                            s['predicted_entity'].values.tolist())]\n",
    "        \n",
    "        agg_func = lambda s: [(s.name, t, p, e) for t, p, e in zip(s['token'].values.tolist(), \n",
    "                                                           s['pos'].values.tolist(), \n",
    "                                                           s['entity'].values.tolist())]\n",
    "        \n",
    "        self.grouped = self.data.groupby('title').apply(agg_func)\n",
    "#         self.grouped_x = self.data.groupby('title').apply(agg_func_x)\n",
    "        \n",
    "        self.sentences = [s for s in self.grouped]\n",
    "#         self.sentences_x = [s for s in self.grouped_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "        word = sent[i][1]\n",
    "        postag = sent[i][2]\n",
    "\n",
    "        features = {\n",
    "            'bias': 1.0, \n",
    "#             'word.lower()': word.lower(), \n",
    "            'word': word,\n",
    "            'word[-3:]': word[-3:],\n",
    "            'word[-2:]': word[-2:],\n",
    "            'word[-1:]': word[-1:],\n",
    "            'word[:3]': word[:3],\n",
    "            'word[:2]': word[:2],\n",
    "            'word[:1]': word[:1],\n",
    "            \n",
    "#             'word.isupper()': word.isupper(),\n",
    "            'word.islower()': word.islower(),\n",
    "            'word.istitle()': word.istitle(),\n",
    "#             'word.isdigit()': word.isdigit(),\n",
    "            'postag': postag,\n",
    "            \n",
    "#             'word_pls_1.lower()': '', \n",
    "            'word_pls_1': '',\n",
    "            'word_pls_1[-3:]': '',\n",
    "            'word_pls_1[-2:]': '',\n",
    "            'word_pls_1[-1:]': '',\n",
    "            'word_pls_1[:3]': '',\n",
    "            'word_pls_1[:2]': '',\n",
    "            'word_pls_1[:1]': '',\n",
    "#             'word_pls_1.isupper()': False,\n",
    "            'word_pls_1.islower()': False,\n",
    "            'word_pls_1.istitle()': False,\n",
    "#             'word_pls_1.isdigit()': False,\n",
    "            'postag_pls_1': '',\n",
    "            \n",
    "#             'word_min_1.lower()': '', \n",
    "            'word_min_1': '',\n",
    "            'word_min_1[-3:]': '',\n",
    "            'word_min_1[-2:]': '',\n",
    "            'word_min_1[-1:]': '',\n",
    "            'word_min_1[:3]': '',\n",
    "            'word_min_1[:2]': '',\n",
    "            'word_min_1[:1]': '',\n",
    "#             'word_min_1.isupper()': False,\n",
    "            'word_min_1.islower()': False,\n",
    "            'word_min_1.istitle()': False,\n",
    "#             'word_min_1.isdigit()': False,\n",
    "            'postag_min_1': '',\n",
    "            \n",
    "            'BOS': True if i == 0 else False,\n",
    "            'EOS': True if i == len(sent)-1 else False,\n",
    "        }\n",
    "        \n",
    "        \n",
    "        if i < len(sent)-1:\n",
    "            word_pls_1 = sent[i+1][1]\n",
    "            postag_pls_1 = sent[i+1][2]\n",
    "            features.update({\n",
    "#                 'word_pls_1.lower()': word_pls_1.lower(), \n",
    "                'word_pls_1': word_pls_1,\n",
    "                'word_pls_1[-3:]': word_pls_1[-3:],\n",
    "                'word_pls_1[-2:]': word_pls_1[-2:],\n",
    "                'word_pls_1[-1:]': word_pls_1[-1:],\n",
    "                'word_pls_1[:3]': word_pls_1[:3],\n",
    "                'word_pls_1[:2]': word_pls_1[:2],\n",
    "                'word_pls_1[:1]': word_pls_1[:1],\n",
    "#                 'word_pls_1.isupper()': word_pls_1.isupper(),\n",
    "                'word_pls_1.islower()': word_pls_1.islower(),\n",
    "                'word_pls_1.istitle()': word_pls_1.istitle(),\n",
    "#                 'word_pls_1.isdigit()': word_pls_1.isdigit(),\n",
    "                'postag_pls_1': postag_pls_1\n",
    "            })\n",
    "        if i > 0:\n",
    "            word_min_1 = sent[i-1][1]\n",
    "            postag_min_1 = sent[i-1][2]\n",
    "            features.update({\n",
    "#                 'word_min_1.lower()': word_min_1.lower(), \n",
    "                'word_min_1': word_min_1,\n",
    "                'word_min_1[-3:]': word_min_1[-3:],\n",
    "                'word_min_1[-2:]': word_min_1[-2:],\n",
    "                'word_min_1[-1:]': word_min_1[-1:],\n",
    "                'word_min_1[:3]': word_min_1[:3],\n",
    "                'word_min_1[:2]': word_min_1[:2],\n",
    "                'word_min_1[:1]': word_min_1[:1],\n",
    "#                 'word_min_1.isupper()': word_min_1.isupper(),\n",
    "                'word_min_1.islower()': word_min_1.islower(),\n",
    "                'word_min_1.istitle()': word_min_1.istitle(),\n",
    "#                 'word_min_1.isdigit()': word_min_1.isdigit(),\n",
    "                'postag_min_1': postag_min_1\n",
    "            })\n",
    "        \n",
    "        return features\n",
    "\n",
    "def sent2features(sent):  \n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for title, token, postag, label in sent]\n",
    "\n",
    "# def sent2labels(sent):\n",
    "#     return [label for title, token, postag, label, pred in sent]\n",
    "\n",
    "def sent2labelspred(sent):\n",
    "    return [pred for title, token, postag, label, pred in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get formated sentences from dataset\n",
    "getter = SentenceGetter(df)\n",
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split train and test\n",
    "X = [x for s in sentences for x in sent2features(s)]\n",
    "y = [x for s in sentences for x in sent2labels(s)]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = df[\"token\"]\n",
    "yy = df[\"entity\"]\n",
    "\n",
    "xx_train, xx_test, yy_train, yy_test = train_test_split(xx, yy, test_size=0.33, random_state=0, shuffle=False)\n",
    "\n",
    "# X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC', 'B-PER', 'I-LOC', 'I-PER']"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = np.unique(y)\n",
    "classes = classes.tolist()\n",
    "\n",
    "# remove the most common tag 'O'(other)\n",
    "new_classes = classes.copy()\n",
    "new_classes.pop()\n",
    "new_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=False)), ('classifier', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vectorizer', DictVectorizer(sparse=False)),\n",
    "    ('classifier', LinearSVC(multi_class='ovr'))\n",
    "])\n",
    "\n",
    "# clf = Pipeline([\n",
    "#     ('vectorizer', DictVectorizer(sparse=False)),\n",
    "#     ('classifier', SVC(kernel='poly', degree=3, C=1.0))\n",
    "# ])\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Training:\n",
      "0.9994619027120103\n",
      "Accuracy of Validation:\n",
      "0.9667904741096788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.80      0.67      0.73         6\n",
      "       B-PER       0.97      0.91      0.94       445\n",
      "       I-LOC       0.00      0.00      0.00         0\n",
      "       I-PER       0.96      0.93      0.94       996\n",
      "\n",
      "   micro avg       0.96      0.92      0.94      1447\n",
      "   macro avg       0.68      0.63      0.65      1447\n",
      "weighted avg       0.96      0.92      0.94      1447\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghost/anaconda3/envs/Playgrounds/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ghost/anaconda3/envs/Playgrounds/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of Training:\")\n",
    "print(clf.score(X_train, y_train))\n",
    "print(\"Accuracy of Validation:\")\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "print(skl_met(y_test, clf.predict(X_test), labels=new_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediksi Salah:\n",
      "152\n"
     ]
    }
   ],
   "source": [
    "y_predict = clf.predict(X_test)\n",
    "evaluation = pd.DataFrame({'real': y_test, 'prediction': y_predict})\n",
    "wrong_pred = []\n",
    "correct_pred = []\n",
    "\n",
    "for i in range(len(evaluation[\"real\"])):\n",
    "    if evaluation[\"real\"][i] != evaluation[\"prediction\"][i]:\n",
    "        wrong_pred.append((i, y_predict[i], y_test[i]))\n",
    "    else:\n",
    "        correct_pred.append((i, y_predict[i], y_test[i]))\n",
    "print(\"Prediksi Salah:\")\n",
    "print(len(wrong_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual testing with data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hadits Muslim Nomor 10</td>\n",
       "      <td>'</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hadits Muslim Nomor 10</td>\n",
       "      <td>Dia</td>\n",
       "      <td>PRON</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hadits Muslim Nomor 10</td>\n",
       "      <td>bertanya</td>\n",
       "      <td>VERB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hadits Muslim Nomor 10</td>\n",
       "      <td>lagi</td>\n",
       "      <td>ADV</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hadits Muslim Nomor 10</td>\n",
       "      <td>'</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    title     token    pos entity\n",
       "0  Hadits Muslim Nomor 10         '  PUNCT      O\n",
       "1  Hadits Muslim Nomor 10       Dia   PRON      O\n",
       "2  Hadits Muslim Nomor 10  bertanya   VERB      O\n",
       "3  Hadits Muslim Nomor 10      lagi    ADV      O\n",
       "4  Hadits Muslim Nomor 10         '  PUNCT      O"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"Datasets/100_test_2.csv\", encoding = \"ISO-8859-1\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetterX(object):\n",
    "    def __init__(self, data):\n",
    "        self.n_sent= 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func_x = lambda s: [(s.name, t, p, e, pr) for t, p, e, pr in zip(s['token'].values.tolist(), \n",
    "                                                           s['pos'].values.tolist(), \n",
    "                                                           s['entity'].values.tolist(), \n",
    "                                                           s['predicted_entity'].values.tolist())]\n",
    "        \n",
    "#         agg_func = lambda s: [(s.name, t, p, e) for t, p, e in zip(s['token'].values.tolist(), \n",
    "#                                                            s['pos'].values.tolist(), \n",
    "#                                                            s['entity'].values.tolist())]\n",
    "        \n",
    "#         self.grouped = self.data.groupby('title').apply(agg_func)\n",
    "        self.grouped_x = self.data.groupby('title').apply(agg_func_x)\n",
    "        \n",
    "#         self.sentences = [s for s in self.grouped]\n",
    "        self.sentences_x = [s for s in self.grouped_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get formated sentences from dataset\n",
    "getter_test = SentenceGetter(df_test)\n",
    "sentences_test = getter_test.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_manual_test = [x for s in sentences_test for x in sent2features(s)]\n",
    "predict = clf.predict(X_manual_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['predicted_entity'] = predict\n",
    "df_test.to_csv('Datasets/predicted.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter_test = SentenceGetterX(df_test)\n",
    "sentences_test_baru = getter_test.sentences_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2labelsx(sent):\n",
    "    return [label for title, token, postag, label, pred in sent]\n",
    "\n",
    "def sent2labelspredx(sent):\n",
    "    return [pred for title, token, postag, label, pred in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4591"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_manual_test = [sent2features(s) for s in sentences_test]\n",
    "y_true_grouped = [sent2labelsx(s) for s in sentences_test_baru]\n",
    "y_pred_grouped = [sent2labelspredx(s) for s in sentences_test_baru]\n",
    "\n",
    "y_true_ungrouped = [x for s in sentences_test_baru for x in sent2labelsx(s)]\n",
    "y_pred_ungrouped = [x for s in sentences_test_baru for x in sent2labelspred(s)]\n",
    "\n",
    "len(y_pred_ungrouped)\n",
    "\n",
    "# print(y_manual_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping token by Hadith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_manual_grouped = [x for s in sentences_test for x in sent2features(s)]\n",
    "y_manual_grouped = [x for s in sentences_test_baru for x in sent2labelsx(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PER ENTITAS\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        PER       0.80      0.83      0.82       447\n",
      "        LOC       0.80      0.67      0.73         6\n",
      "\n",
      "avg / total       0.80      0.83      0.81       453\n",
      "\n",
      "PER TOKEN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.80      0.67      0.73         6\n",
      "       B-PER       0.97      0.91      0.94       446\n",
      "       I-LOC       0.00      0.00      0.00         0\n",
      "       I-PER       0.96      0.93      0.94       996\n",
      "\n",
      "   micro avg       0.96      0.92      0.94      1448\n",
      "   macro avg       0.68      0.63      0.65      1448\n",
      "weighted avg       0.96      0.92      0.94      1448\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghost/anaconda3/envs/Playgrounds/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ghost/anaconda3/envs/Playgrounds/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# print(classification_report(y_manual_test, y_pred_manual_test))\n",
    "\n",
    "print(\"PER ENTITAS\")\n",
    "print(seq_met(y_true_grouped, y_pred_grouped))\n",
    "print(\"PER TOKEN\")\n",
    "print(skl_met(y_true_ungrouped, y_pred_ungrouped, labels=new_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5]"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz = [1,2,3,4,5]\n",
    "\n",
    "zz[-3:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
